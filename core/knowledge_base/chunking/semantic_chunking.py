
class SemanticChunking(BaseChunking):
    def __init__(self, config: Dict):
        """
        Initialize the semantic chunking class.
        
        :param config: Dictionary containing chunking configuration parameters
        """
        super().__init__(config)
        try:
            # If threshold_type is None, use 'percentile' as the default
            threshold_type = config.get("semantic_breakpoint_threshold_type")
            self.breakpoint_threshold_type = 'percentile' if threshold_type is None else threshold_type
            self.breakpoint_threshold_amount = config.get("semantic_breakpoint_threshold_amount", 90)
            self.min_chunk_size = config.get("semantic_min_chunk_size", constants.DEFAULT_MINIMUM_CHUNK_SIZE)

            # Initialize embeddings with correct Azure settings
            try:
                self.embeddings = AzureOpenAIEmbeddings(
                    azure_endpoint=settings.azure_openai_endpoint,
                    azure_deployment=settings.azure_embedding_deployment,
                    api_version=settings.azure_embedding_api_version,
                    api_key=settings.azure_openai_api_key,
                )
                logger.info("Successfully initialized Azure OpenAI Embeddings for semantic chunking")
            except Exception as e:
                logger.error(f"Failed to initialize AzureOpenAIEmbeddings: {str(e)}")
                logger.error("Azure OpenAI Settings used: "
                           f"endpoint={settings.azure_openai_endpoint}, "
                           f"deployment={settings.azure_embedding_deployment}, "
                           f"api_version={settings.azure_embedding_api_version}")
                raise ValueError(f"Embeddings initialization failed: {str(e)}")

        except Exception as e:
            logger.error(f"Error initializing SemanticChunking: {str(e)}")
            raise

    def _chunk_content(self, content: str) -> List[str]:
        """
        Implements semantic chunking logic.
        
        :param content: The preprocessed content to chunk
        :return: List of string chunks
        """
        try:
            if not content:
                logger.warning("Empty content provided for semantic chunking")
                return []

            logger.info(f"Starting semantic chunking with threshold_type={self.breakpoint_threshold_type}, "
                       f"threshold_amount={self.breakpoint_threshold_amount}, "
                       f"min_chunk_size={self.min_chunk_size}")

            text_splitter = SemanticChunker(
                embeddings=self.embeddings,
                breakpoint_threshold_type=self.breakpoint_threshold_type,
                breakpoint_threshold_amount=self.breakpoint_threshold_amount,
                min_chunk_size=self.min_chunk_size
            )
            
            docs = text_splitter.create_documents([content])
            chunks = [doc.page_content.strip() for doc in docs if doc.page_content.strip()]
            
            if not chunks:
                logger.warning("No chunks generated by semantic chunking")
                return []
            
            logger.info(f"Successfully generated {len(chunks)} chunks using semantic chunking")
            return chunks

        except Exception as e:
            logger.error(f"Error in semantic chunking: {str(e)}")
            raise ValueError(f"Semantic chunking failed: {str(e)}")